<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <title>How to New a Post - InnerPeace Chou&#8217;s Blog</title>
    <meta name="author" content="Wubai Zhou">
    <meta name="description" content="The first post rake new_post[“YYYY-MM-DD-post-title.markdown”] rake preview localhost:4000 Take a look also to rake generate (generates the content &hellip;">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="canonical" href="http://zhouwubai.github.io/blog/2015/02/04/how-to-new-a-post">
    <link href="/favicon.png" rel="icon">
    <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
    <link href="/atom.xml" rel="alternate" title="InnerPeace Chou's Blog" type="application/atom+xml">
    <script src="/javascripts/modernizr-2.0.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script>
    !window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))
    </script>
    <script src="/javascripts/octopress.js" type="text/javascript"></script>
    <script type="text/javascript" src="/javascripts/customer.js"></script>
    <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
    <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } });
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i=0; i
        < all.length; i +=1 ) { all[i].SourceElement().parentNode.className +=' has-jax' ; } }); </script>
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
            </script>
            <script type="text/javascript">
            $(function() {
                get_recent_posts(5);
            });
            </script>
</head>

<body>
    <header role="banner">
        <hgroup>
            <h1><a href="/">InnerPeace Chou&#8217;s Blog</a></h1>
            <h2>谦虚做人，积极做事.</h2>
        </hgroup>
    </header>
    <nav role="navigation">
        <ul class="subscription" data-subscription="rss">
            <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
        </ul>
        <form action="https://www.google.com/search" method="get">
            <fieldset role="search">
                <input type="hidden" name="q" value="site:zhouwubai.github.io" />
                <input class="search" type="text" name="q" results="0" placeholder="Search" />
            </fieldset>
        </form>
        <ul class="main-navigation">
            <li><a href="/about">About</a></li>
            <li><a href="/blog">Blog</a></li>
        </ul>
    </nav>
    <div id="main">
        <div id="content">
            <div>
                <article class="hentry" role="article">
                    <header>
                        <h1 class="entry-title">LDA implementation roadmap 1: distribution</h1>
                        <p class="meta">
                            <time datetime="2015-05-15T04:25:22-05:00" pubdate data-updated="true"></time>
                        </p>
                    </header>
                    <div class="entry-content">
                        <p>Latent Dirichlet Allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes the following generative process for each document w in a corpus D:
                          <ol>
                            <li>Choose $N \sim Poisson(\xi)$</li>
                            <li>Choose $\theta \sim Dir(\alpha)$, generalized multivariate beta distribution</li>
                            <li>For each of the N words $w_n$
                              <ol>
                                <li>Choose a topic $z_n \sim Multinomial(\theta)$.</li>
                                <li>Choose a word $w_n$ from $p(w_n|z_n,\beta)$, a multinomial probability conditioned on the topic $z_n$, more precisely, it is categorical distribution.</li>
                              </ol>
                            </li>
                          </ol>
                        </p>

                        <p>LDA involves many fundamental concepts and algorithms in Machine Learning area such as some basic and popular probability distributions, EM algorithm, variational inference which greatly trigger my interests to implement it. This article I will first write something about some related distributions and their origin functions.</p>
                        
                        <h4 id="the-first-post">Distributions and functions</h4>
                        <h5 id="gamma-function">Gamma Function</h5>
                        <p>Gamma function is the basic for lots of functions and probabilities, such as Beta function, Beta distribution, Dirichlet distribution and so on. In mathamatics, the gamma function is an extention of of the factorial function. With positive real integer, that is $\Gamma(n) = (n-1)!$; For complex numbers with a positive real part, it is defined as：
                            <script type="math/tex; mode=display">
                            \Gamma(t) = \int_0^{\infty}{x^{t-1}e^{-x}dx}
                            </script>
                             Following is the gamma function along part of the real axis. Notice that the function domain is usually restricted in $[1, \infty]$ in most of real application.</p>

                        <p>
                        <a href="http://en.wikipedia.org/wiki/Gamma_function#/media/File:Gamma_plot.svg"><img src="http://upload.wikimedia.org/wikipedia/commons/5/52/Gamma_plot.svg" width="450" /></a>
                        </p>

                        <h5 id="gamma-distribution">Gamma Distribution</h5>
                        <p>The Gamma Distribution $X \sim \Gamma(\alpha, \beta) \equiv Gammma(\alpha,\beta)$ is a two parameter family of continuous probability distributions. The are three different parametrization in common use:
                            <ol>
                                <li>With a shape parameter $k$ and a scale parameter $\theta$.</li>
                                <li>With a shape parameter $\alpha = k$ and an inverse scale parameter $\beta = 1/\theta$, called a rate parameter.</li>
                                <li>With a shape parameter $k$ and a mean parameter $mu = k / \beta$</li>
                            </ol>
                        The corresponding density function in the shape-rate parametrization is:
                        <script type="math/tex; mode=display">
                        g(x;\alpha, \beta) = \frac{\beta^{\alpha} x^{\alpha-1} e^{-\lambda x}}{\Gamma(\alpha)}
                        </script>
                        Its PDF plot is shown as following:
                        </p>

                        <p>
                        <a href="http://en.wikipedia.org/wiki/Gamma_distribution#/media/File:Gamma_distribution_pdf.svg"><img src="http://upload.wikimedia.org/wikipedia/commons/e/e6/Gamma_distribution_pdf.svg" width="450" /></a>
                        </p>

                        <p>The parameterization with $\alpha$ and $\beta$ is commonly used in Bayesian statistic, where gamma is used as a conjugate prior distribution for exponential distribution and Possion distribution. Moreover, common exponential distribution $X \sim Exp(X) \sim Gamma(1,\lambda)$ and chi-square distribution are special cases of gamma distribution.The chi-squared distribution (also chi-square or $\chi^{2}$-distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.</p>

                        <h5>Logarithmic expectation</h5>
                        <script type="math/tex; model=display">
                            E[ln(X)] = \phi(\alpha) - ln(\beta)
                        </script>
                        <p>in which $\phi$ is the digamma function defined as the logarithmic derivative of the gamma function, i.e., $\phi(x) = \frac{d}{dx}ln \Gamma(x)$. This property will be used in varational inference process.</p>

                        <h5 id="beta-function">Beta function</h5>
                        Beta function is defined as: 
                        <script type="math/tex; mode=display">
                            B(x,y)  =  \int_0^1 t^{x-1}(1-t)^{y-1} dt
                        </script>

                        <p> The relationship between Gamma function and Beta function is: $\Gamma(x)\Gamma(y) = \Gamma(x+y)B(x,y)$. It can be derived as following :
                        <script type="math/tex; mode=display">
                            \begin{eqnarray*}
                                \Gamma(x)\Gamma(y) & = & \int_0^{\infty}\int_0^{\infty}{e^{-u-v}\mu^{x-1}\nu^{y-1}} d\mu d\nu \\
                                                   & = & \int_{z=0}^{\infty}\int_{t=0}^1{e^{-t}(zt)^{x-1}(z(1-t))^{y-1}} J|z,t| dzdt \\
                                                   & = & \int_{z=0}^{\infty}{e^{-t}z^{x+y-1}} dz \int_0^1{t^{x-1}(1-t)^{y-1}} dt \\
                                                   & = & \Gamma(x+y)B(x,y)
                            \end{eqnarray*}
                        </script>
                        where we putting $u=zt, v = z(1-t)$ and $J|z,t| = z$ is the absolute value of the Jacobian determinant of $z$ and $t$. Beta(x,y) becomes large if one of the arguments close to 1 and small if x and y are both away from 0. Following is its function plot.
                        </p>

                        <p><a href="http://en.wikipedia.org/wiki/Beta_function#/media/File:Beta_function_on_real_plane.png"><img src="http://upload.wikimedia.org/wikipedia/commons/f/f4/Beta_function_on_real_plane.png" width="450"></img></a></p>

                        <h5 id="beta-distribution">Beta distribution</h5>
                        <p>In the bayesian inference, the beta distribution is the conjugate prior probability distribution for Bernouli (one trial for two class, categorical distribution is a generalized Bernouli distribution for multiple class), binomial (sum of multiple indepedent Bernouli trial and multinomial distribution is a generalized binomial distribution) and geometric distribution. The beta distribution is a suitable model for the random behavior percentage and proportions. The PDF of beta distribution is as follows:
                        <script type="math/text; mode=display">
                            f(x;\alpha,\beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta - 1} =\frac{1}{B(\alpha,\beta) x^{\alpha-1}(1-x)^{\beta - 1}
                        </script>
                        The beta function is a normalization constant. Following shows its PDF plot:
                        </p>

                        <p><a href="http://en.wikipedia.org/wiki/Beta_distribution#/media/File:Beta_distribution_pdf.svg"><img src="http://upload.wikimedia.org/wikipedia/commons/f/f3/Beta_distribution_pdf.svg" width="450"></a></p>

                        <h5 id="dirichlet-distribution">Dirichlet Distribution</h5>
                        <p>Dirichlet distribution is a multivariate generalized Beta distribution and it is often used as prior distribution in Bayesian statistics, and in fact it is the conjugate prior of the categorical distribution and multinomial distribution. That is, its probability density function returns the belief the probabilities of K rival events are $x_i$ given that each event has been observed $\alpha_i - 1$ times. The infinite-dimensional generalization of Dirichlet distribution is Dirichlet process. Folowing is its PDF: 
                        <script type="math/text; mode=display">
                            f(x_1,\ldots,x_{K-1};\alpha_1,\ldots,\alpha_K) = \frac{1}{B(\alpha)} \Pi_1^K{x_i^{\alpha_i}}
                        </script>
                        Where $\alpha = \sum_1^K{\alpha_i}$
                        </p>

                        <h5>How to generate Dirichlet distribution</h5>
                        <p>For independently distributed Gamma distributions:
                        <script type="math/text; mode=display">
                            Y_1 \sim Gamma(\alpha_1, \theta), \ldots, Y_K = Gamma(\alpha_K, \theta)
                        </script>
                        We have: 
                        <script type="math/text; mode=display">
                            \begin{eqnarray}
                                V & = & \sum_1^K Y_i \sim Gamma() \\
                                X & = & (X_1,\ldots, X_K) = (\frac{Y_1}{V},\ldots,\frac{Y_K}{V}) \sim Dir(\alpha_1, \ldots, \alpha_K)
                            \end{eqnarray}
                        </script>
                        which means we can easily get Dirchlet distribution from Gamma distribution. Following shows its PDF plot:
                        </p>

                        <p><a href="http://en.wikipedia.org/wiki/Dirichlet_distribution#/media/File:Dirichlet_distributions.png"></a><img src="http://upload.wikimedia.org/wikipedia/commons/3/3e/Dirichlet_distributions.png" width="450"></p>

                    </div>
                    <footer>
                        <p class="meta">
                            <span class="byline author vcard">Posted by <span class="fn">Wubai Zhou</span></span>
                            <time datetime="2015-02-04T04:25:22-05:00" pubdate data-updated="true"></time>
                        </p>
                        <div class="sharing">
                            <a href="//twitter.com/share" class="twitter-share-button" data-url="http://zhouwubai.github.io/blog/2015/02/04/how-to-new-a-post/" data-via="" data-counturl="http://zhouwubai.github.io/blog/2015/02/04/how-to-new-a-post/">Tweet</a>
                        </div>
                        <p class="meta">
                        </p>
                    </footer>
                </article>
            </div>
            <aside class="sidebar">
                <section>
                    <img src="/images/aboutme.jpg" />
                    <p>
                        computer science.
                    </p>
                </section>
                <section>
                    <h1>Recent Posts</h1>
                    <ul id="recent_posts">
                    </ul>
                </section>
            </aside>
        </div>
    </div>
    <footer role="contentinfo">
        <p>
            Copyright &copy; 2015 - Wubai Zhou -
            <span class="credit">Designed by <a href="http://stchangg.com">@stchangg</a></span> -
            <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
        </p>
    </footer>
    <script type="text/javascript">
    (function() {
        var twitterWidgets = document.createElement('script');
        twitterWidgets.type = 'text/javascript';
        twitterWidgets.async = true;
        twitterWidgets.src = '//platform.twitter.com/widgets.js';
        document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
    </script>
</body>

</html>